{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOJzOPrhCRe6hX+wbGXe2jU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"CfTGnd6jigJo"},"outputs":[],"source":["## "]},{"cell_type":"markdown","source":["### Learning Pytorch with examples\n"],"metadata":{"id":"GBdMvaonixS1"}},{"cell_type":"code","source":["import numpy as np\n","import math \n","\n","# Create random input and output data\n","x = np.linspace(-math.pi, math.pi, 2000)\n","y = np.sin(x)\n","\n","## Randomly initializing the weights\n","a = np.random.randn()\n","b = np.random.randn()\n","c = np.random.randn()\n","d = np.random.randn()\n","\n","learning_rate = 1e-6\n","for t in range(2000):\n","  ## Forward Pass: Compute predicted Y\n","  y_pred = a + b * x + c * x ** 2 + d * x ** 3\n","\n","  ## Compute and print loss\n","  loss = np.square(y_pred - y).sum()\n","  if t % 100 == 99:\n","    print(t, loss)\n","\n","  ## Backprop to compute the gradients a, b, c, d with respect to loss\n","  grad_y_pred = 2.0 * (y_pred - y)\n","  grad_a = grad_y_pred.sum()\n","  grad_b = (grad_y_pred * x).sum()\n","  grad_c = (grad_y_pred * x ** 2).sum()\n","  grad_d = (grad_y_pred * x ** 3).sum()\n","\n","  ## Update the weights\n","  a -= learning_rate * grad_a\n","  b -= learning_rate * grad_b\n","  c -= learning_rate * grad_c\n","  d -= learning_rate * grad_d\n","\n","print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"],"metadata":{"id":"3FKF3JoJi4Du","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681673990348,"user_tz":300,"elapsed":326,"user":{"displayName":"Venkata SivaSai Pavan Kumar Lottala","userId":"07636081166347936613"}},"outputId":"c098f941-5c51-4834-de04-dab1a7fa01d4"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["99 265.8470837967111\n","199 186.6959978877714\n","299 132.04388400614553\n","399 94.26677004534797\n","499 68.1272297564381\n","599 50.021987357937604\n","699 37.46928554330488\n","799 28.757939972816114\n","899 22.706807631307186\n","999 18.499742208802324\n","1099 15.572217323534616\n","1199 13.533354950275985\n","1299 12.112242921943594\n","1399 11.120934463530023\n","1499 10.428918090182147\n","1599 9.945483205860874\n","1699 9.607526759531835\n","1799 9.371113502678337\n","1899 9.205628498921559\n","1999 9.089721732002197\n","Result: y = -0.01637039798917668 + 0.8623558528914168 x + 0.0028241660577313305 x^2 + -0.09412907529122137 x^3\n"]}]},{"cell_type":"markdown","source":["## Pytorch Tensors:\n","A pytorch tensor is conceptually identical to a numpy array. A tensor is an n-dimensional array and pytorch provides many functions for operatin on the tensors. The tensors can keep track of a computational graph and gradients. "],"metadata":{"id":"BEtI0lDxlhih"}},{"cell_type":"code","source":["import torch\n","import math\n","\n","dtype = torch.float\n","device = torch.device(\"cpu\")\n","\n","## Create random input and ouput data \n","x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n","y = torch.sin(x)\n","\n","## Randomly initialize the weights\n","a = torch.randn((), device=device, dtype=dtype)\n","b = torch.randn((), device=device, dtype=dtype)\n","c = torch.randn((), device=device, dtype=dtype)\n","d = torch.randn((), device=device, dtype=dtype)\n","\n","learning_rate = 1e-6\n","for t in range(2000):\n","    # Forward pass: compute predicted y\n","    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n","\n","    # Compute and print loss\n","    loss = (y_pred - y).pow(2).sum().item()\n","    if t % 100 == 99:\n","        print(t, loss)\n","\n","    # Backprop to compute gradients of a, b, c, d with respect to loss\n","    grad_y_pred = 2.0 * (y_pred - y)\n","    grad_a = grad_y_pred.sum()\n","    grad_b = (grad_y_pred * x).sum()\n","    grad_c = (grad_y_pred * x ** 2).sum()\n","    grad_d = (grad_y_pred * x ** 3).sum()\n","\n","    # Update weights using gradient descent\n","    a -= learning_rate * grad_a\n","    b -= learning_rate * grad_b\n","    c -= learning_rate * grad_c\n","    d -= learning_rate * grad_d\n","\n","\n","print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tyeo34qBlG91","executionInfo":{"status":"ok","timestamp":1681674419233,"user_tz":300,"elapsed":5209,"user":{"displayName":"Venkata SivaSai Pavan Kumar Lottala","userId":"07636081166347936613"}},"outputId":"a229c348-efb4-4aaa-d307-e99c74e53290"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["99 2213.094482421875\n","199 1510.6475830078125\n","299 1033.2193603515625\n","399 708.3646240234375\n","499 487.0755920410156\n","599 336.1634826660156\n","699 233.1282958984375\n","799 162.70040893554688\n","899 114.5052719116211\n","999 81.48645782470703\n","1099 58.83917999267578\n","1199 43.28791046142578\n","1299 32.59718322753906\n","1399 25.239635467529297\n","1499 20.170434951782227\n","1599 16.673999786376953\n","1699 14.259830474853516\n","1799 12.591140747070312\n","1899 11.436576843261719\n","1999 10.6368989944458\n","Result: y = -0.0386975035071373 + 0.8353735208511353 x + 0.006675963290035725 x^2 + -0.09029107540845871 x^3\n"]}]},{"cell_type":"markdown","source":["### Autograds Functions:\n","The autograd package in PyTorch provides eactly this functionality. When using this autograd these are the steps that takes place:\n","1. The forward pass of the network will define a computational graph\n","2. Nodes in the graph will be tensors\n","3. Edges will be functions that produce output tensors from input tensors.\n","4. Back Propagation through the graph allows us to easily compute the gradients\n","\n"],"metadata":{"id":"4fQK9nConFDn"}},{"cell_type":"code","source":["import torch \n","import math\n","\n","dtype = torch.float\n","device = torch.device(\"cpu\")\n","\n","## Create the tensors to hold the input and outputs\n","## By default the requires_grad = False. Which indicates that we do not need to\n","## Compute the gradients\n","x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n","y = torch.sin(x)\n","\n","## Setting the variables\n","a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n","b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n","c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n","d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n","\n","## Learning rate:\n","learning_rate = 1e-6\n","for t in range(2000):\n","  ## Forward pass: Compute the predicted y \n","  y_pred = a + b * x + c * x ** 2 + d * x ** 3\n","\n","  ## Compute the loss\n","  loss = (y_pred - y).pow(2).sum()\n","  if t % 100 == 99:\n","    print(t, loss.item())\n","\n","  ## Compute the loss\n","  loss.backward()\n","\n","  ### Updating the weighs\n","  with torch.no_grad():\n","    a -= learning_rate * a.grad\n","    b -= learning_rate * b.grad\n","    c -= learning_rate * c.grad\n","    d -= learning_rate * d.grad\n","\n","    a.grad = None\n","    b.grad = None \n","    c.grad = None\n","    d.grad = None\n","\n","print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xX9JcgOhmyXk","executionInfo":{"status":"ok","timestamp":1681675220442,"user_tz":300,"elapsed":980,"user":{"displayName":"Venkata SivaSai Pavan Kumar Lottala","userId":"07636081166347936613"}},"outputId":"84b72c83-36a3-4e70-886b-8ecde7015d29"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["99 4938.86572265625\n","199 3292.8994140625\n","299 2197.46240234375\n","399 1468.121337890625\n","499 982.318115234375\n","599 658.5852661132812\n","699 442.75091552734375\n","799 298.78076171875\n","899 202.6961669921875\n","999 138.53448486328125\n","1099 95.6647720336914\n","1199 67.00367736816406\n","1299 47.82981491088867\n","1399 34.99409866333008\n","1499 26.395509719848633\n","1599 20.631017684936523\n","1699 16.763721466064453\n","1799 14.16709041595459\n","1899 12.422221183776855\n","1999 11.248696327209473\n","Result: y = -0.02854892797768116 + 0.816612720489502 x + 0.004925166256725788 x^2 + -0.0876225084066391 x^3\n"]}]},{"cell_type":"markdown","source":["### Defining the new autograd functions\n","These two functions that operate on tensors:\n","1. The forward function computes output tensors from input tensors\n","2. The backward function recieves the gradient of the output tensors with \n","respect to some scalr value and compute the gradient of the input tensors \n","with respect to that same scalar value\n"," "],"metadata":{"id":"wYghOo-WqQsn"}},{"cell_type":"code","source":["import torch\n","import math\n","\n","class LegendrePolynomial3(torch.autograd.Function):\n","  \"\"\"\n","  We are sub classing the torch.autograd.Function and implementing \n","  Forward and Backward Passes which operates on tensors.\n","  \"\"\"   \n","  @staticmethod\n","  def forward(ctx, input):\n","    \"\"\"\n","    In forward pass we receive a tensor containing the input and return \n","    a tensor containing the output. CTX is a context object that can be \n","    used to stash information for backward computation. \n","    You can cache arbitrary objects for use in the backward pass using the ctx.save_for_backward\n","    \"\"\"\n","    ctx.save_for_backward(input)\n","    return 0.5 * (5 * input ** 3 - 3 * input)\n","\n","  @staticmethod\n","  def backward(ctx, grad_output):\n","    input, = ctx.saved_tensors\n","    return grad_output * 1.5 * (5 * input ** 2 - 1)\n","\n","\n","dtype = torch.float\n","device = torch.device(\"cpu\")\n","\n","x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n","y = torch.sin(x)\n","\n","# Create random Tensors for weights. For this example, we need\n","# 4 weights: y = a + b * P3(c + d * x), these weights need to be initialized\n","# not too far from the correct result to ensure convergence.\n","# Setting requires_grad=True indicates that we want to compute gradients with\n","# respect to these Tensors during the backward pass.\n","a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n","b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\n","c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n","d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n","\n","## Learning rate \n","learing_rate = 5e-6\n","for t in range(2000):\n","  P3 = LegendrePolynomial3.apply\n","\n","  y_pred = a + b * P3(c + d * x)\n","\n","  ## Compute and print loss\n","  loss = (y_pred - y).pow(2).sum()\n","  if t%100 == 99:\n","    print(t, loss.item())\n","\n","  ## Using autograd to compute the backward pass\n","  loss.backward()\n","\n","  ## Updating the weights\n","  with torch.no_grad():\n","    a -= learning_rate * a.grad\n","    b -= learning_rate * b.grad\n","    c -= learning_rate * c.grad\n","    d -= learning_rate * d.grad\n","\n","    ## Manually zero the gradients after updating the weights\n","    a.grad = None\n","    b.grad = None \n","    c.grad = None \n","    d.grad = None\n","\n","print(f\"Results: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x) \")\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0CK0yu5up28u","executionInfo":{"status":"ok","timestamp":1681676386868,"user_tz":300,"elapsed":1286,"user":{"displayName":"Venkata SivaSai Pavan Kumar Lottala","userId":"07636081166347936613"}},"outputId":"f037da6b-f7e8-43c3-f138-9870d1b19a1e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["99 283.5877685546875\n","199 262.7139587402344\n","299 243.45401000976562\n","399 225.67848205566406\n","499 209.26953125\n","599 194.1190185546875\n","699 180.1279754638672\n","799 167.20550537109375\n","899 155.26817321777344\n","999 144.23941040039062\n","1099 134.04885864257812\n","1199 124.63192749023438\n","1299 115.928466796875\n","1399 107.8837661743164\n","1499 100.4473876953125\n","1599 93.57269287109375\n","1699 87.21684265136719\n","1799 81.34027099609375\n","1899 75.90643310546875\n","1999 70.88170623779297\n","Results: y = -6.169046610354778e-12 + -1.668351173400879 * P3(1.3746551119631079e-10 + 0.25102949142456055 x) \n"]}]},{"cell_type":"markdown","source":["### Pytorch nn Module\n","1. These are modules that receives the input tensors and compute the output tensors\n","2. They also hold internal state such as tensors containing learnable parameters\n","3. The nn package also defines a set of useful loss functions that are commonly used when training neural networks"],"metadata":{"id":"Uc-MtaEsur5c"}},{"cell_type":"code","source":["import torch \n","import math\n","\n","x = torch.linspace(-math.pi, math.pi, 2000)\n","y = torch.sin(x)\n","\n","### For this example the output y is a linear function of (x, x^2, X^3)\n","### Let's prepare the tensor (x, x^2, X^3)\n","p  = torch.tensor([1 , 2, 3])\n","xx = x.unsqueeze(-1).pow(p)\n","\n","## In the above code, x.unsqueeze(-1) has shape (2000, 1) and p has a shape\n","## (3, ) for this case the broad casting semantics will apply to obtain a tensor\n","## of shape (2000, 3)\n","\n","### Use the nn package to define our model as a sequence of layers. nn.Sequential\n","### Is a modul that contains other modules.\n","model = torch.nn.Sequential(\n","    torch.nn.Linear(3, 1),\n","    torch.nn.Flatten(0, 1)\n",")\n","\n","\n","## The nn package also contains the loss functions \n","loss_fn = torch.nn.MSELoss(reduction=\"sum\")\n","\n","learning_rate=1e-6\n","for t in range(2000):\n","  y_pred = model(xx)\n","\n","  ## Compute and print loss. We pass tensors containing the predicted and \n","  ## true values of y and the loss functions returns a tensor containing the loss\n","  loss = loss_fn(y_pred, y)\n","  if t % 100 == 99:\n","    print(t, loss.item())\n","\n","  model.zero_grad()\n","\n","  ## Loss backward\n","  loss.backward()\n","\n","  ## Update the weights\n","  with torch.no_grad():\n","    for param in model.parameters():\n","      param -= learning_rate * param.grad\n","\n","linear_layer = model[0]\n","# For linear layer, its parameters are stored as `weight` and `bias`.\n","print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rV-M1Dh7uD85","executionInfo":{"status":"ok","timestamp":1681677437112,"user_tz":300,"elapsed":1778,"user":{"displayName":"Venkata SivaSai Pavan Kumar Lottala","userId":"07636081166347936613"}},"outputId":"fb8b53ab-1334-43f9-de21-c6f5eba3258c"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["99 125.43793487548828\n","199 86.0801010131836\n","299 60.011688232421875\n","399 42.743412017822266\n","499 31.303125381469727\n","599 23.722946166992188\n","699 18.699705123901367\n","799 15.370560646057129\n","899 13.163719177246094\n","999 11.700592041015625\n","1099 10.730386734008789\n","1199 10.086929321289062\n","1299 9.660059928894043\n","1399 9.376856803894043\n","1499 9.188905715942383\n","1599 9.064146995544434\n","1699 8.981307029724121\n","1799 8.926287651062012\n","1899 8.889738082885742\n","1999 8.865448951721191\n","Result: y = -0.00223319954238832 + 0.8503051400184631 x + 0.0003852641093544662 x^2 + -0.0924149602651596 x^3\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"zkMKtYccyN4F"},"execution_count":null,"outputs":[]}]}